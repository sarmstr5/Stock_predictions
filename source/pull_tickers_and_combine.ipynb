{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import requests\n",
    "\n",
    "# Due to yahoo finance discontinueing their API, had to use a work around\n",
    "# this package uses deprecated functions\n",
    "# import fix_yahoo_finance as yf\n",
    "# yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sp500_tickers(fn = \"wiki_sp500tickers.pickle\", save_pickle=True, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"in save tickers\")\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    wiki_table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    \n",
    "    # now we have the wiki table with all the data we want the tickers to pull from yahoo/google\n",
    "    wiki_tickers = []\n",
    "    for row in wiki_table.findAll('tr')[1:]:     # dont need header column\n",
    "        ticker = row.findAll('td')[0].text      # grab table data text\n",
    "        wiki_tickers.append(ticker)\n",
    "    \n",
    "    # save list\n",
    "    if verbose:\n",
    "        print(\"Tickers have been found saving to disk\")\n",
    "    if save_pickle:\n",
    "        with open(fn, \"wb\") as f:\n",
    "            pickle.dump(wiki_tickers, f)\n",
    "    else:\n",
    "        with open(fn, \"w\") as f:\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow([\"Ticker#\", \"Ticker\"])\n",
    "            count = 0\n",
    "            for ticker in wiki_tickers:\n",
    "                wr.writerow([count, ticker])\n",
    "                count += 1\n",
    "    \n",
    "    return wiki_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_online(repull_sp500=False, ticker_fn='wiki_sp500tickers.pickle', save_pickle=True, finance_source='google', verbose=False):\n",
    "    if verbose:\n",
    "        print('Loading tickers')\n",
    "        \n",
    "    if repull_sp500:\n",
    "        tickers = save_sp500_tickers(ticker_fn, save_pickle, verbose)\n",
    "    # read sp500 list from disk\n",
    "    else:\n",
    "        with open(ticker_fn, \"rb\") as f:\n",
    "            if save_pickle:\n",
    "                tickers = pickle.load(f)\n",
    "            else:\n",
    "                tickers = []\n",
    "                rd = csv.reader(f, deliminter='')\n",
    "                for row in rd:\n",
    "                    tickers.append(row)\n",
    "         \n",
    "    # make directory if not made        \n",
    "    if not os.path.exists(\"data/stock_data\"):\n",
    "        if verbose:\n",
    "            print(\"making stock_data dir\")\n",
    "        os.makedirs(\"data/stock_data\")\n",
    "    \n",
    "    # data dates \n",
    "    start = dt.datetime(2000, 1, 1)\n",
    "    end = dt.datetime.today()\n",
    "    stocks_not_found = []\n",
    "    stocks_not_found_fn = \"missing_stocks_{}\".format(finance_source)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"saving stock data to file\")\n",
    "        \n",
    "    for ticker in tickers:\n",
    "        if not os.path.exists('data/stock_data/{}'.format(ticker)):\n",
    "            try:\n",
    "                df = pdr.DataReader(ticker, finance_source, start, end)\n",
    "                df.to_csv('data/stock_data/{}.csv'.format(ticker))\n",
    "            \n",
    "            except:\n",
    "                print(\"had a connection error and skipped {}\".format(ticker))\n",
    "                stocks_not_found.append(ticker)\n",
    "        else:\n",
    "            print('did not replace {}'.format(ticker))\n",
    "            \n",
    "    if verbose:\n",
    "        print(\"finished with saving stock data\")\n",
    "\n",
    "    with open(stocks_not_found_fn, \"wb\") as f:\n",
    "        pickle.dump(stocks_not_found, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_data_from_online(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sp500_data(tickers_fn=\"wiki_sp500tickers.pickle\", combined_fn=\"allSP500_closed_google.csv\", \n",
    "                       missing_stocks_fn='missing_stocks_google.pickle'):\n",
    "    \n",
    "    with open(tickers_fn, \"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    with open(missing_stocks_fn, \"rb\") as f:\n",
    "        missing_tickers = pickle.load(f)\n",
    "        \n",
    "    sp500_df = pd.DataFrame()\n",
    "    for count, ticker in enumerate(tickers):\n",
    "        if count % 10 ==0:\n",
    "            print(count)\n",
    "            \n",
    "        if ticker in missing_tickers:\n",
    "            print(\"ticker {} has been skipped\".format(ticker))\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df = pd.read_csv('data/stock_data/{}.csv'.format(ticker))\n",
    "            df.set_index('Date', inplace=True)\n",
    "            \n",
    "            # Change close column name to ticker and drop other columns\n",
    "            df.rename(columns={'Close':ticker}, inplace=True)\n",
    "            df.drop([\"Open\", \"High\", \"Low\", \"Volume\"], 1, inplace=True)\n",
    "            \n",
    "            if sp500_df.empty:\n",
    "               sp500_df = df\n",
    "            else:\n",
    "                sp500_df = sp500_df.join(df, how=\"outer\")\n",
    "        except:\n",
    "            raise \n",
    "            \n",
    "    sp500_df.tocsv('data/{}'.format(combined_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'missing_stocks_google.pickle'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-78f3a7f1da90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcombine_sp500_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-cf22dedaabc3>\u001b[0m in \u001b[0;36mcombine_sp500_data\u001b[1;34m(tickers_fn, combined_fn, missing_stocks_fn)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mtickers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_stocks_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mmissing_tickers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'missing_stocks_google.pickle'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "combine_sp500_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RemoteDataError",
     "evalue": "Unable to read URL: http://www.google.com/finance/historical?startdate=Jan+01%2C+2010&output=csv&enddate=Jan+27%2C+2013&q=LMT",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDataError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6c8ebfa34b6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2013\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LMT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'google'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shane.armstrong\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py\u001b[0m in \u001b[0;36mDataReader\u001b[1;34m(name, data_source, start, end, retry_count, pause, session, access_key)\u001b[0m\n\u001b[0;32m    131\u001b[0m                                  \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                                  \u001b[0mretry_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                                  session=session).read()\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdata_source\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"enigma\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shane.armstrong\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             df = self._read_one_data(self.url,\n\u001b[1;32m--> 157\u001b[1;33m                                      params=self._get_params(self.symbols))\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;31m# Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shane.armstrong\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_read_one_data\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;34m\"\"\" read one data from specified URL \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_url_as_StringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shane.armstrong\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_read_url_as_StringIO\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mOpen\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mand\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shane.armstrong\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_get_response\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"?\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRemoteDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unable to read URL: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRemoteDataError\u001b[0m: Unable to read URL: http://www.google.com/finance/historical?startdate=Jan+01%2C+2010&output=csv&enddate=Jan+27%2C+2013&q=LMT"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime(2013, 1, 27)\n",
    "f = web.DataReader(\"LMT\", 'google', start, end)\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}